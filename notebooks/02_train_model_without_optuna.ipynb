{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSDK_7Dj3YH9",
        "outputId": "1ddde303-c2e8-4612-8100-36cf689ffa7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded from SQLite\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnancies</th>\n",
              "      <th>glucose</th>\n",
              "      <th>blood_pressure</th>\n",
              "      <th>skin_thickness</th>\n",
              "      <th>insulin</th>\n",
              "      <th>bmi</th>\n",
              "      <th>diabetes_pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pregnancies  glucose  blood_pressure  skin_thickness  insulin   bmi  \\\n",
              "0            6    148.0            72.0            35.0    125.0  33.6   \n",
              "1            1     85.0            66.0            29.0    125.0  26.6   \n",
              "2            8    183.0            64.0            29.0    125.0  23.3   \n",
              "3            1     89.0            66.0            23.0     94.0  28.1   \n",
              "4            0    137.0            40.0            35.0    168.0  43.1   \n",
              "\n",
              "   diabetes_pedigree  age  outcome  \n",
              "0              0.627   50        1  \n",
              "1              0.351   31        0  \n",
              "2              0.672   32        1  \n",
              "3              0.167   21        0  \n",
              "4              2.288   33        1  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to SQLite DB\n",
        "conn = sqlite3.connect(\"../data/diabetes.db\")\n",
        "\n",
        "# Load dataset from DB\n",
        "df = pd.read_sql(\"\"\"\n",
        "SELECT \n",
        "    p.pregnancies,\n",
        "    p.glucose,\n",
        "    p.blood_pressure,\n",
        "    p.skin_thickness,\n",
        "    p.insulin,\n",
        "    p.bmi,\n",
        "    p.diabetes_pedigree,\n",
        "    p.age,\n",
        "    l.outcome\n",
        "FROM patients p\n",
        "JOIN labels l\n",
        "ON p.patient_id = l.patient_id\n",
        "\"\"\", conn)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "print(\"Data loaded from SQLite\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df.drop(\"outcome\", axis=1)\n",
        "y = df[\"outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logistic_regression F1-score: 0.5455\n",
            "random_forest F1-score: 0.6600\n",
            "svm F1-score: 0.6000\n",
            "gradient_boosting F1-score: 0.6263\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "os.makedirs(\"../models\", exist_ok=True)\n",
        "\n",
        "models = {\n",
        "    \"logistic_regression\": LogisticRegression(max_iter=1000),\n",
        "    \"random_forest\": RandomForestClassifier(random_state=42),\n",
        "    \"svm\": SVC(),\n",
        "    \"gradient_boosting\": GradientBoostingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    preds = model.predict(X_test_scaled)\n",
        "    f1 = f1_score(y_test, preds)\n",
        "\n",
        "    joblib.dump(model, f\"../models/{name}_no_pca_no_optuna.pkl\")\n",
        "\n",
        "    print(f\"{name} F1-score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory:\n",
            "c:\\Users\\SRIVARDHANBABAVEMULA\\Desktop\\major project\\housing_app_fall25\\notebooks\n",
            "\n",
            "Does DB file exist?\n",
            "True\n",
            "\n",
            "Absolute path of DB:\n",
            "c:\\Users\\SRIVARDHANBABAVEMULA\\Desktop\\major project\\housing_app_fall25\\data\\diabetes.db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sqlite3\n",
        "\n",
        "print(\"Current working directory:\")\n",
        "print(os.getcwd())\n",
        "\n",
        "print(\"\\nDoes DB file exist?\")\n",
        "print(os.path.exists(\"../data/diabetes.db\"))\n",
        "\n",
        "print(\"\\nAbsolute path of DB:\")\n",
        "print(os.path.abspath(\"../data/diabetes.db\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "DatabaseError",
          "evalue": "Execution failed on sql '\nSELECT \n    p.pregnancies,\n    p.glucose,\n    p.blood_pressure,\n    p.skin_thickness,\n    p.insulin,\n    p.bmi,\n    p.diabetes_pedigree,\n    p.age,\n    l.outcome\nFROM patients p\nJOIN labels l\nON p.patient_id = l.patient_id\n': no such table: patients",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
            "\u001b[1;31mOperationalError\u001b[0m: no such table: patients",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/diabetes.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load data from database\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    p.pregnancies,\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m    p.glucose,\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    p.blood_pressure,\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    p.skin_thickness,\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m    p.insulin,\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m    p.bmi,\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m    p.diabetes_pedigree,\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m    p.age,\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m    l.outcome\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124mFROM patients p\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124mJOIN labels l\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124mON p.patient_id = l.patient_id\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m, conn)\n\u001b[0;32m     24\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded from SQLite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    707\u001b[0m             sql,\n\u001b[0;32m    708\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    709\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    710\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    711\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    712\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    713\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    714\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    715\u001b[0m         )\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT \n    p.pregnancies,\n    p.glucose,\n    p.blood_pressure,\n    p.skin_thickness,\n    p.insulin,\n    p.bmi,\n    p.diabetes_pedigree,\n    p.age,\n    l.outcome\nFROM patients p\nJOIN labels l\nON p.patient_id = l.patient_id\n': no such table: patients"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect(\"../data/diabetes.db\")\n",
        "\n",
        "# Load data from database\n",
        "df = pd.read_sql(\"\"\"\n",
        "SELECT \n",
        "    p.pregnancies,\n",
        "    p.glucose,\n",
        "    p.blood_pressure,\n",
        "    p.skin_thickness,\n",
        "    p.insulin,\n",
        "    p.bmi,\n",
        "    p.diabetes_pedigree,\n",
        "    p.age,\n",
        "    l.outcome\n",
        "FROM patients p\n",
        "JOIN labels l\n",
        "ON p.patient_id = l.patient_id\n",
        "\"\"\", conn)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "print(\"Data loaded from SQLite.\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tables found in database:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "conn = sqlite3.connect(\"../data/diabetes.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Tables found in database:\")\n",
        "print(tables)\n",
        "\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "quIhilm65orD",
        "outputId": "daf0111b-5a1f-4c55-95e2-d46b5143ee37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Collecting mlflow<3\n",
            "  Downloading mlflow-2.22.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Collecting mlflow-skinny==2.22.4 (from mlflow<3)\n",
            "  Downloading mlflow_skinny-2.22.4-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.1.2)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.1.6)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (1.17.2)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow<3)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow<3)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow<3)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.10)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (3.10.0)\n",
            "Requirement already satisfied: pandas!=2.3.0,<3 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (1.6.1)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow<3) (2.0.44)\n",
            "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.22.4->mlflow<3)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.4->mlflow<3)\n",
            "  Downloading databricks_sdk-0.74.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (0.118.3)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (1.37.0)\n",
            "Collecting packaging<25 (from mlflow-skinny==2.22.4->mlflow<3)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==2.22.4->mlflow<3) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow<3) (1.3.10)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow<3) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow<3) (3.1.4)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow<3)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow<3)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow<3) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow<3) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.3.0,<3->mlflow<3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.3.0,<3->mlflow<3) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow<3) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow<3) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<3) (3.3.0)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==2.22.4->mlflow<3) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.4->mlflow<3) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.4->mlflow<3) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.4->mlflow<3) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.4->mlflow<3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.4->mlflow<3) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.4->mlflow<3) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow<3) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.4->mlflow<3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.4->mlflow<3) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.4->mlflow<3) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==2.22.4->mlflow<3) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.4->mlflow<3) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.4->mlflow<3) (4.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.4->mlflow<3) (0.6.1)\n",
            "Downloading mlflow-2.22.4-py3-none-any.whl (29.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.22.4-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading databricks_sdk-0.74.0-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.2/764.2 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, graphql-core, cachetools, gunicorn, graphql-relay, docker, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 6.2.2\n",
            "    Uninstalling cachetools-6.2.2:\n",
            "      Successfully uninstalled cachetools-6.2.2\n",
            "Successfully installed cachetools-5.5.2 databricks-sdk-0.74.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.22.4 mlflow-skinny-2.22.4 packaging-24.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "c153986371424ee38bdb3d3e2c51ed94",
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install xgboost lightgbm \"mlflow<3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "DatabaseError",
          "evalue": "Execution failed on sql '\nSELECT \n    p.pregnancies,\n    p.glucose,\n    p.blood_pressure,\n    p.skin_thickness,\n    p.insulin,\n    p.bmi,\n    p.diabetes_pedigree,\n    p.age,\n    l.outcome\nFROM patients p\nJOIN labels l\nON p.patient_id = l.patient_id\n': no such table: patients",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
            "\u001b[1;31mOperationalError\u001b[0m: no such table: patients",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/diabetes.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load data from DB\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m    p.pregnancies,\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m    p.glucose,\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    p.blood_pressure,\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m    p.skin_thickness,\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m    p.insulin,\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m    p.bmi,\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m    p.diabetes_pedigree,\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m    p.age,\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m    l.outcome\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124mFROM patients p\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124mJOIN labels l\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124mON p.patient_id = l.patient_id\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m, conn)\n\u001b[0;32m     24\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded from SQLite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    707\u001b[0m             sql,\n\u001b[0;32m    708\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    709\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    710\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    711\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    712\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    713\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    714\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    715\u001b[0m         )\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\SRIVARDHANBABAVEMULA\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT \n    p.pregnancies,\n    p.glucose,\n    p.blood_pressure,\n    p.skin_thickness,\n    p.insulin,\n    p.bmi,\n    p.diabetes_pedigree,\n    p.age,\n    l.outcome\nFROM patients p\nJOIN labels l\nON p.patient_id = l.patient_id\n': no such table: patients"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to database\n",
        "conn = sqlite3.connect(\"../data/diabetes.db\")\n",
        "\n",
        "# Load data from DB\n",
        "df = pd.read_sql(\"\"\"\n",
        "SELECT \n",
        "    p.pregnancies,\n",
        "    p.glucose,\n",
        "    p.blood_pressure,\n",
        "    p.skin_thickness,\n",
        "    p.insulin,\n",
        "    p.bmi,\n",
        "    p.diabetes_pedigree,\n",
        "    p.age,\n",
        "    l.outcome\n",
        "FROM patients p\n",
        "JOIN labels l\n",
        "ON p.patient_id = l.patient_id\n",
        "\"\"\", conn)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "print(\"Data loaded from SQLite.\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRFqzWx-55CJ",
        "outputId": "c29141ec-c5ad-4588-c0df-e2cfe379a509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/housing_fall2025\n"
          ]
        }
      ],
      "source": [
        "base_folder = \"/content/drive/MyDrive/Colab Notebooks/housing_fall2025\"\n",
        "%cd \"{base_folder}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "m1pqkWQ53TFh",
        "outputId": "070a405b-7ffb-4945-e9ce-723ddf5dda26"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-384e642c-d7fb-4066-b11c-e6364738b4e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>block_id</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322</td>\n",
              "      <td>126</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401</td>\n",
              "      <td>1138</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-122.24</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1467</td>\n",
              "      <td>190.0</td>\n",
              "      <td>496</td>\n",
              "      <td>177</td>\n",
              "      <td>7.2574</td>\n",
              "      <td>352100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1274</td>\n",
              "      <td>235.0</td>\n",
              "      <td>558</td>\n",
              "      <td>219</td>\n",
              "      <td>5.6431</td>\n",
              "      <td>341300.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1627</td>\n",
              "      <td>280.0</td>\n",
              "      <td>565</td>\n",
              "      <td>259</td>\n",
              "      <td>3.8462</td>\n",
              "      <td>342200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-384e642c-d7fb-4066-b11c-e6364738b4e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-384e642c-d7fb-4066-b11c-e6364738b4e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-384e642c-d7fb-4066-b11c-e6364738b4e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   block_id  longitude  latitude  housing_median_age  total_rooms  \\\n",
              "0         0    -122.23     37.88                41.0          880   \n",
              "1         1    -122.22     37.86                21.0         7099   \n",
              "2         2    -122.24     37.85                52.0         1467   \n",
              "3         3    -122.25     37.85                52.0         1274   \n",
              "4         4    -122.25     37.85                52.0         1627   \n",
              "\n",
              "   total_bedrooms  population  households  median_income  median_house_value  \\\n",
              "0           129.0         322         126         8.3252            452600.0   \n",
              "1          1106.0        2401        1138         8.3014            358500.0   \n",
              "2           190.0         496         177         7.2574            352100.0   \n",
              "3           235.0         558         219         5.6431            341300.0   \n",
              "4           280.0         565         259         3.8462            342200.0   \n",
              "\n",
              "  ocean_proximity  \n",
              "0        NEAR BAY  \n",
              "1        NEAR BAY  \n",
              "2        NEAR BAY  \n",
              "3        NEAR BAY  \n",
              "4        NEAR BAY  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "conn = sqlite3.connect(f\"{base_folder}/data/housing.db\")\n",
        "housing = pd.read_sql_query(\n",
        "    \"\"\"\n",
        "    SELECT\n",
        "        b.block_id,\n",
        "        b.longitude,\n",
        "        b.latitude,\n",
        "        s.housing_median_age,\n",
        "        s.total_rooms,\n",
        "        s.total_bedrooms,\n",
        "        s.population,\n",
        "        s.households,\n",
        "        s.median_income,\n",
        "        s.median_house_value,\n",
        "        op.name AS ocean_proximity\n",
        "    FROM block AS b\n",
        "    JOIN block_housing_stats AS s\n",
        "        ON s.block_id = b.block_id\n",
        "    JOIN ocean_proximity AS op\n",
        "        ON op.ocean_proximity_id = b.ocean_proximity_id\n",
        "    ORDER BY b.block_id\n",
        "    \"\"\",\n",
        "    conn,\n",
        ")\n",
        "conn.close()\n",
        "\n",
        "housing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTqNkkbR33iG",
        "outputId": "463058a0-4a86-4aa1-b191-dd24eb27442d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ STEP 1: Preprocessing pipeline created.\n",
            "✓ STEP 2: Stratified split done. Train size: 16512, Test size: 4128\n",
            "✓ STEP 3: 4 baseline model pipelines defined.\n",
            "✓ STEP 4: MLflow configured.\n",
            "\n",
            "================================================================================\n",
            "Training baseline model: ridge\n",
            "================================================================================\n",
            "ridge (no PCA) CV MAE: $51,151.03\n",
            "ridge (no PCA) Test MAE: $52,350.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'ridge_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'ridge_pipeline'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training baseline model: histgradientboosting\n",
            "================================================================================\n",
            "histgradientboosting (no PCA) CV MAE: $30,702.81\n",
            "histgradientboosting (no PCA) Test MAE: $30,702.41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'histgradientboosting_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'histgradientboosting_pipeline'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training baseline model: xgboost\n",
            "================================================================================\n",
            "xgboost (no PCA) CV MAE: $28,831.45\n",
            "xgboost (no PCA) Test MAE: $28,465.49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'xgboost_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'xgboost_pipeline'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training baseline model: lightgbm\n",
            "================================================================================\n",
            "lightgbm (no PCA) CV MAE: $29,422.96\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003784 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4651\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score 206333.518653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lightgbm (no PCA) Test MAE: $29,684.98\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "Registered model 'lightgbm_pipeline' already exists. Creating a new version of this model...\n",
            "Created version '5' of model 'lightgbm_pipeline'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ STEP 5: All 4 baseline models trained and logged.\n",
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: ridge\n",
            "================================================================================\n",
            "ridge_with_pca CV MAE: $56,480.26\n",
            "ridge_with_pca Test MAE: $56,738.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'ridge_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'ridge_pipeline_with_pca'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: histgradientboosting\n",
            "================================================================================\n",
            "histgradientboosting_with_pca CV MAE: $38,567.13\n",
            "histgradientboosting_with_pca Test MAE: $37,990.38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'histgradientboosting_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'histgradientboosting_pipeline_with_pca'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: xgboost\n",
            "================================================================================\n",
            "xgboost_with_pca CV MAE: $38,057.22\n",
            "xgboost_with_pca Test MAE: $37,367.20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "Registered model 'xgboost_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'xgboost_pipeline_with_pca'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Training PCA-augmented model: lightgbm\n",
            "================================================================================\n",
            "lightgbm_with_pca CV MAE: $37,690.64\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015235 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2295\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 206333.518653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lightgbm_with_pca Test MAE: $37,631.80\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "Registered model 'lightgbm_pipeline_with_pca' already exists. Creating a new version of this model...\n",
            "Created version '4' of model 'lightgbm_pipeline_with_pca'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ STEP 7: All 4 PCA models trained and logged.\n",
            "\n",
            "================================================================================\n",
            "GLOBAL BEST MODEL (ACROSS 8 CANDIDATES)\n",
            "================================================================================\n",
            "Global best model key: xgboost\n",
            "Global best CV MAE:    $28,831.45\n",
            "Global best Test MAE:  $28,465.49\n",
            "Uses PCA:               False\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Saving and reloading GLOBAL best model...\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Model saved to /content/drive/MyDrive/Colab Notebooks/housing_fall2025/models/global_best_model.pkl\n",
            "\n",
            "Done:\n",
            "- GLOBAL best model key: xgboost\n",
            "- GLOBAL best CV MAE:    $28,831.45\n",
            "- GLOBAL best Test MAE:  $28,465.49\n",
            "Elapsed time: 2 minutes and 50.34 seconds\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# FULL PIPELINE:\n",
        "# - Build preprocessing\n",
        "# - Stratified train/test split\n",
        "# - Train & log 4 models WITHOUT PCA (Ridge, HGB, XGBoost, LightGBM)\n",
        "# - Train & log 4 models WITH PCA (preprocessing + PCA(0.95) + model)\n",
        "# - Pick GLOBAL best among 8 models by Test MAE\n",
        "# - Save, load, and compare the global best model\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "import joblib\n",
        "\n",
        "# Import shared components\n",
        "from housing_pipeline import (\n",
        "    build_preprocessing,\n",
        "    make_estimator_for_name,\n",
        ")\n",
        "\n",
        "start_time = time.monotonic()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Build Full ML Preprocessing Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "preprocessing = build_preprocessing()\n",
        "print(\"✓ STEP 1: Preprocessing pipeline created.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Split Data into Stratified Train and Test Sets\n",
        "# =============================================================================\n",
        "\n",
        "housing[\"income_cat\"] = pd.cut(\n",
        "    housing[\"median_income\"],\n",
        "    bins=[0, 1.5, 3.0, 4.5, 6, np.inf],\n",
        "    labels=[1, 2, 3, 4, 5],\n",
        ")\n",
        "\n",
        "train_set, test_set = train_test_split(\n",
        "    housing,\n",
        "    test_size=0.20,\n",
        "    stratify=housing[\"income_cat\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "for df in (train_set, test_set):\n",
        "    df.drop(\"income_cat\", axis=1, inplace=True)\n",
        "\n",
        "X_train = train_set.drop([\"block_id\", \"median_house_value\"], axis=1).copy()\n",
        "y_train = train_set[\"median_house_value\"].copy()\n",
        "\n",
        "X_test = test_set.drop([\"block_id\", \"median_house_value\"], axis=1).copy()\n",
        "y_test = test_set[\"median_house_value\"].copy()\n",
        "\n",
        "print(f\"✓ STEP 2: Stratified split done. Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Define 4 Model Pipelines (WITHOUT PCA)\n",
        "# =============================================================================\n",
        "\n",
        "models = {}\n",
        "for name in [\"ridge\", \"histgradientboosting\", \"xgboost\", \"lightgbm\"]:\n",
        "    est = make_estimator_for_name(name)\n",
        "    models[name] = make_pipeline(preprocessing, est)\n",
        "\n",
        "print(\"✓ STEP 3: 4 baseline model pipelines defined.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Configure MLflow (e.g., Dagshub) via .env\n",
        "# =============================================================================\n",
        "\n",
        "load_dotenv(\n",
        "    dotenv_path=\"/content/gdrive/MyDrive/Colab Notebooks/housing_fall2025/notebooks/.env\",\n",
        "    override=True\n",
        ")\n",
        "\n",
        "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
        "MLFLOW_TRACKING_USERNAME = os.getenv(\"MLFLOW_TRACKING_USERNAME\")\n",
        "MLFLOW_TRACKING_PASSWORD = os.getenv(\"MLFLOW_TRACKING_PASSWORD\")\n",
        "\n",
        "if MLFLOW_TRACKING_USERNAME:\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = MLFLOW_TRACKING_USERNAME\n",
        "if MLFLOW_TRACKING_PASSWORD:\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = MLFLOW_TRACKING_PASSWORD\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "mlflow.set_experiment(\"median_house_pricing_multi_model\")\n",
        "\n",
        "print(\"✓ STEP 4: MLflow configured.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Train, Evaluate, and Log 4 Baseline Models (NO PCA)\n",
        "# =============================================================================\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, pipeline in models.items():\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Training baseline model: {name}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    # Compute CV MAE before fitting on full training set\n",
        "    cv_scores = cross_val_score(\n",
        "        pipeline, X_train, y_train,\n",
        "        cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
        "    )\n",
        "    cv_mae = -cv_scores.mean()\n",
        "    print(f\"{name} (no PCA) CV MAE: ${cv_mae:,.2f}\")\n",
        "\n",
        "    # Fit on full training set\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    test_mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\"{name} (no PCA) Test MAE: ${test_mae:,.2f}\")\n",
        "\n",
        "    results[name] = {\"pipeline\": pipeline, \"test_mae\": test_mae, \"cv_mae\": cv_mae}\n",
        "\n",
        "    with mlflow.start_run(run_name=f\"{name}_baseline\"):\n",
        "        mlflow.log_param(\"model_family\", name)\n",
        "        mlflow.log_param(\"uses_pca\", False)\n",
        "\n",
        "        est_step_name = list(pipeline.named_steps.keys())[-1]\n",
        "        est = pipeline.named_steps[est_step_name]\n",
        "        est_params = {f\"{est_step_name}__{k}\": v for k, v in est.get_params().items()}\n",
        "        mlflow.log_params(est_params)\n",
        "\n",
        "        mlflow.log_metric(\"cv_MAE\", cv_mae)\n",
        "        mlflow.log_metric(\"test_MAE\", test_mae)\n",
        "\n",
        "        signature = infer_signature(X_train, pipeline.predict(X_train))\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=pipeline,\n",
        "            artifact_path=\"housing_model\",\n",
        "            signature=signature,\n",
        "            input_example=X_train,\n",
        "            registered_model_name=f\"{name}_pipeline\",\n",
        "        )\n",
        "\n",
        "print(\"\\n✓ STEP 5: All 4 baseline models trained and logged.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: Train, Evaluate, and Log PCA Versions of ALL 4 Models\n",
        "# =============================================================================\n",
        "\n",
        "pca_results = {}\n",
        "\n",
        "for name in models.keys():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Training PCA-augmented model: {name}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    est = make_estimator_for_name(name)\n",
        "\n",
        "    pca_pipeline = make_pipeline(\n",
        "        preprocessing,\n",
        "        PCA(n_components=0.95),\n",
        "        est,\n",
        "    )\n",
        "\n",
        "    # Compute CV MAE before fitting on full training set\n",
        "    cv_scores_pca = cross_val_score(\n",
        "        pca_pipeline, X_train, y_train,\n",
        "        cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1\n",
        "    )\n",
        "    cv_mae_pca = -cv_scores_pca.mean()\n",
        "    print(f\"{name}_with_pca CV MAE: ${cv_mae_pca:,.2f}\")\n",
        "\n",
        "    # Fit on full training set\n",
        "    pca_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred_pca = pca_pipeline.predict(X_test)\n",
        "    test_mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
        "\n",
        "    model_key = f\"{name}_with_pca\"\n",
        "    pca_results[model_key] = {\n",
        "        \"pipeline\": pca_pipeline,\n",
        "        \"test_mae\": test_mae_pca,\n",
        "        \"cv_mae\": cv_mae_pca,\n",
        "    }\n",
        "\n",
        "    print(f\"{model_key} Test MAE: ${test_mae_pca:,.2f}\")\n",
        "\n",
        "    with mlflow.start_run(run_name=model_key):\n",
        "        mlflow.log_param(\"model_family\", name)\n",
        "        mlflow.log_param(\"uses_pca\", True)\n",
        "\n",
        "        est_step_name = list(pca_pipeline.named_steps.keys())[-1]\n",
        "        est_step = pca_pipeline.named_steps[est_step_name]\n",
        "        est_params = {f\"{est_step_name}__{k}\": v for k, v in est_step.get_params().items()}\n",
        "        mlflow.log_params(est_params)\n",
        "\n",
        "        pca_step = pca_pipeline.named_steps[\"pca\"]\n",
        "        mlflow.log_param(\"pca__n_components\", pca_step.n_components)\n",
        "\n",
        "        mlflow.log_metric(\"cv_MAE\", cv_mae_pca)\n",
        "        mlflow.log_metric(\"test_MAE\", test_mae_pca)\n",
        "\n",
        "        signature_pca = infer_signature(X_train, pca_pipeline.predict(X_train))\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=pca_pipeline,\n",
        "            artifact_path=\"housing_model_with_pca\",\n",
        "            signature=signature_pca,\n",
        "            input_example=X_train,\n",
        "            registered_model_name=f\"{name}_pipeline_with_pca\",\n",
        "        )\n",
        "\n",
        "print(\"\\n✓ STEP 7: All 4 PCA models trained and logged.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: Choose GLOBAL Best Model (with or without PCA)\n",
        "# =============================================================================\n",
        "\n",
        "all_results = {}\n",
        "all_results.update(results)\n",
        "all_results.update(pca_results)\n",
        "\n",
        "global_best_name = min(all_results, key=lambda k: all_results[k][\"test_mae\"])\n",
        "global_best_mae = all_results[global_best_name][\"test_mae\"]\n",
        "global_best_cv_mae = all_results[global_best_name][\"cv_mae\"]\n",
        "global_best_pipeline = all_results[global_best_name][\"pipeline\"]\n",
        "\n",
        "uses_pca = \"with_pca\" in global_best_name\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GLOBAL BEST MODEL (ACROSS 8 CANDIDATES)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Global best model key: {global_best_name}\")\n",
        "print(f\"Global best CV MAE:    ${global_best_cv_mae:,.2f}\")\n",
        "print(f\"Global best Test MAE:  ${global_best_mae:,.2f}\")\n",
        "print(f\"Uses PCA:               {uses_pca}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 9: Save, Load, and Compare the GLOBAL Best Model\n",
        "# =============================================================================\n",
        "\n",
        "def save_model(model, filename=\"global_best_model.pkl\"):\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"✓ Model saved to {filename}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"Saving and reloading GLOBAL best model...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "save_model(global_best_pipeline, filename=f\"{base_folder}/models/global_best_model.pkl\")\n",
        "\n",
        "print(\"\\nDone:\")\n",
        "print(f\"- GLOBAL best model key: {global_best_name}\")\n",
        "print(f\"- GLOBAL best CV MAE:    ${global_best_cv_mae:,.2f}\")\n",
        "print(f\"- GLOBAL best Test MAE:  ${global_best_mae:,.2f}\")\n",
        "\n",
        "end_time = time.monotonic()\n",
        "elapsed_time = end_time - start_time\n",
        "minutes = int(elapsed_time // 60)\n",
        "seconds = elapsed_time % 60\n",
        "print(f\"Elapsed time: {minutes} minutes and {seconds:.2f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
